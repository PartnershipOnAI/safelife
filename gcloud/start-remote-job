#!/usr/bin/env python3

import os
import argparse
import subprocess as sp
import textwrap
import shutil


parser = argparse.ArgumentParser(description=textwrap.dedent("""
    Start a new SafeLife training run.

    This performs the following steps:

    1. Clone the repo to a new temporary folder.
    2. Copy all source files over to the remote machine using rsync.
    3. ssh into the remote machine and
        a. create an alias to the soon-to-be-created data folder;
        b. start a tmux session that shares the training job's name;
        c. listens on the appropriate local port for tensorboard updates;
        d. starts training via the `start-training` script.

    Note that the `start-training` script will shut down the remote instance
    with a 10 minute lag before the script exits, whether via error or normal
    completion. The shutdown is there to prevent machine from idling and
    running up large bills, while the lag is designed so that it's possible
    to abort the shutdown by sshing into the remote machine and running
    `sudo shutdown -c`. This comes in handy when the script fails at startup
    due to user error or a bug.
    """), formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument('instance_name', help="name of the gcloud instance")
parser.add_argument('job_name', help="a unique name for this training job")
parser.add_argument('--port', default='6006',
    help="local port used to monitor tensorboard")
parser.add_argument('-b', '--branch',
    help="git branch or tag to use for this run. "
    "If not supplied, a new commit is made with all unstaged files added, "
    "and it is given the tag `job-<job_name>`. The commit is then reverted "
    "to keep HEAD in the same state as it started in.")
args, remaining_args = parser.parse_known_args()

src_dir = '~/' + args.job_name
data_dir = '~/{job_name}/data/{job_name}/'.format(job_name=args.job_name)

quiet = {'stdout': sp.DEVNULL, 'stderr': sp.DEVNULL}

if args.branch:
    result = sp.run(["git", "show", args.branch], **quiet)
    if result.returncode == 128:
        print(f"Branch '{args.branch}' does not exist.")
        exit(1)
    elif result.returncode:
        print("Unknown git error.")
        exit(1)
    git_tag = args.branch
else:
    git_tag = f'job-{args.job_name}'
    result = sp.run(["git", "show", git_tag], **quiet)
    if result.returncode == 0:
        print(f"Tag or branch '{git_tag}' already exists.")
        print("Cannot make a new tag.")
        exit(1)
    elif result.returncode != 128:
        print("Unknown git error.")
        exit(1)

    if sp.run("git status -s", shell=True, stdout=sp.PIPE).stdout:
        # There are changes in the working directory.
        # Stash changes, apply the stash, commit and tag, rollback, then
        # unstash the changes such that the working directory is left in the
        # exact same condition as it started in.
        print("adding uncommitted changes...")
        print(f"adding tag {git_tag}...")
        sp.run(f"""
            git stash push -ku
            git stash push
            git stash apply 1
            git add .
            git commit -m "job {args.job_name}"
            git tag {git_tag}
            git reset --hard HEAD~1
            git stash pop
            git add .
            git stash pop
            """, shell=True, **quiet)
    else:
        # No changes to be committed. Just add a tag.
        print(f"adding tag {git_tag}...")
        sp.run(f"git tag {git_tag}", shell=True, **quiet)

# Copy over the data
safety_dir = os.path.abspath(os.path.join(__file__, '../../'))
ssh_cmd = os.path.abspath(os.path.join(__file__, '../ssh'))

print("cloning temporary repo...")
result = sp.run(
    f"git clone --no-local --depth 1 --branch {git_tag} . ./tmp-repo",
    shell=True, **quiet)
if result.returncode != 0:
    print("Error cloning the repo.")
    exit(1)

print("syncing repo to the cloud...")
result = sp.run([
    'rsync', '--rsh', ssh_cmd, '-ra',
    './tmp-repo/', args.instance_name + ':' + src_dir])
shutil.rmtree("./tmp-repo")
if result.returncode != 0:
    exit(1)

print("starting job...")
# Run the script, tunneling tensorboard to the specified port.
result = sp.run([
    ssh_cmd, args.instance_name, '-L', args.port + ':localhost:6006',

    # Set the path variable to include conda, which contains the latest
    # versions of python, torch, etc.
    # This is very specific to our particular gcloud setup.
    "PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:$PATH; "

    # Create a "current_job" directory. This is handy for re-entering the job.
    f"ln -nsf {src_dir} ~/current_job; "

    # Install dependencies.
    "sudo apt-get install ffmpeg --yes; "
    f"pip install -r {src_dir}/requirements.txt; "

    # And run the training script!
    # This uses tmux to prevent it from dying on hangup.
    # Note that if this session is already running, tmux should prevent us
    # from running it again.
    'export PATH=/opt/anaconda3/bin:/opt/anaconda3/condabin:"$PATH" ; '
    f"tmux new-session -s {args.job_name} "
    f"{src_dir}/start-training {data_dir} --ensure-gpu " + ' '.join(remaining_args)
    + " ; bash "
])
