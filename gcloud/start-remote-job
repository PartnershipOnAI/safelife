#!/usr/bin/env python3

import os
import argparse
import subprocess as sp
import shutil
import tempfile
import textwrap
import time
import uuid


parser = argparse.ArgumentParser(description=textwrap.dedent("""
    Start a new SafeLife training run.

    This performs the following steps:

    1. Clone the repo to a new temporary folder.
    2. Copy all source files over to the remote machine using rsync.
    3. ssh into the remote machine and
        a. create an alias to the soon-to-be-created data folder;
        b. start a tmux session that shares the training job's name;
        c. listens on the appropriate local port for tensorboard updates;
        d. starts training via the `start-training` script.

    Note that the `start-training` script will shut down the remote instance
    with a 3 minute lag before the script exits, whether via error or normal
    completion. The shutdown is there to prevent machine from idling and
    running up large bills, while the lag is designed so that it's possible
    to abort the shutdown by sshing into the remote machine and running
    `sudo shutdown -c`. This comes in handy when the script fails at startup
    due to user error or a bug.
    """), formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument('instance_name', help="name of the gcloud instance")
parser.add_argument('job_name', help="a unique name for this training job")
parser.add_argument('--port', help="local port used to monitor tensorboard")
parser.add_argument('--commit-id',
    help="If supplied, the job is run from the specified commit id (or tag or "
    "branch). Otherwise, changes in the working tree are stashed and the "
    "job is run from the stashed commit. That is, the job is run using the "
    "code as it currently exists, minus any untracked files.")
parser.add_argument('--tag', nargs='?', const="auto",
    help="If set, the commit is given the specified tag. "
    "If auto, the tag is autogenerated with the form 'job-[job_name]'.")
parser.add_argument('--no-shutdown', action="store_true",
    help="By default, the remote machine will shutdown when the job is "
    "complete. This flag prevents that from happening.")
parser.add_argument('--sweep', action="store_true",
    help="Specifies that the job is part of a wandb sweep. "
    'The job name should be of the form "entity/project/sweepid"')
args, remaining_args = parser.parse_known_args()

if args.sweep:
    sweep_id = args.job_name
    args.job_name = 'sweep-' + sweep_id
src_dir = '~/' + args.job_name
data_dir = '~/{job_name}/data/{job_name}/'.format(job_name=args.job_name)

quiet = {'stdout': sp.DEVNULL, 'stderr': sp.DEVNULL}

# There are a couple files which we typically want to keep out of git(hub),
# but we do want to add them to the remote machine if they're available.

if args.commit_id:
    commit_id = args.commit_id
    new_commit = False
else:
    result = sp.run("git stash create", shell=True, stdout=sp.PIPE, check=True)
    new_commit = commit_id = result.stdout.decode()
    if not new_commit:
        # means there were no stashed chages. Use HEAD instead.
        result = sp.run("git rev-parse HEAD", shell=True, stdout=sp.PIPE)
        commit_id = result.stdout.decode()

print(f"Setting job to commit id '{commit_id}'...")

if args.tag == 'auto':
    git_tag = f"job-{args.job_name}"
elif args.tag:
    git_tag = args.tag
else:
    # create a temporary tag
    git_tag = f'job-{uuid.uuid4()}'
result = sp.run(f"git tag {git_tag} {commit_id}", shell=True, **quiet)
if result.returncode == 128:
    print(f"Tag '{git_tag}' already exists. Aborting.")
    exit(1)
elif result.returncode != 0:
    print(f"Failed to add tag '{git_tag}'.")
    exit(1)
else:
    print(f"Adding tag '{git_tag}'...")


# Copy over the data
safety_dir = os.path.abspath(os.path.join(__file__, '../../'))
ssh_cmd = os.path.abspath(os.path.join(__file__, '../ssh'))

print("Cloning temporary repo...")

try:
    tmp_repo = tempfile.TemporaryDirectory(prefix="safelife-job-")
    result = sp.run(
        f"git clone --no-local --single-branch --branch {git_tag} . {tmp_repo.name}",
        shell=True, **quiet)
    if result.returncode != 0:
        print("Error cloning the repo.")
        exit(1)

    # Make the remote of the cloned repo match the remote of this one
    result = sp.run("git remote get-url origin", shell=True, stdout=sp.PIPE)
    if result.returncode == 0:
        remote_url = result.stdout.decode().strip()
        sp.run(
            f"cd {tmp_repo.name} && git remote set-url origin {remote_url}",
            shell=True, **quiet)
    else:
        print("Error getting the remote url.")

    if new_commit:
        # Roll back the new commit.
        # This is so that the cloned repo matches the state of the current
        # repo, including untracked files and HEAD commit id.
        sp.run(
            f"cd {tmp_repo.name} && git reset --soft HEAD~1",
            shell=True, **quiet)

    # Also copy over some files that are not (necessarily) in source control.
    extra_files = [
        'run-info.md',
        'run-notes.txt',
        'wandb/settings',
    ]
    for extra_file in extra_files:
        src_file = os.path.join(safety_dir, extra_file)
        dst_file = os.path.join(tmp_repo.name, extra_file)
        if os.path.exists(src_file):
            os.makedirs(os.path.dirname(dst_file), exist_ok=True)
            shutil.copyfile(src_file, dst_file)

    print("Creating", src_dir)
    if result.returncode != 0:
        exit(1)
    time.sleep(3)
    result = sp.run([ssh_cmd, args.instance_name, "mkdir", "-p", src_dir])
    if result.returncode != 0:
        exit(1)
    sync_command = [
        'rsync', '--rsh', ssh_cmd, '-ra',
        tmp_repo.name + '/', args.instance_name + ':' + src_dir
    ]
    print("Syncing repo to the cloud...", sync_command)
    result = sp.run(sync_command)
    if result.returncode != 0:
        exit(1)
finally:
    if not args.tag:
        print("Removing temporary tag...")
        sp.run(f"git tag -d {git_tag}", shell=True, **quiet)
    print("Cleaning temporary repo...")
    tmp_repo.cleanup()

print("Starting job...")

if args.sweep:
    training_cmd = f"wandb agent {sweep_id}"
else:
    training_cmd = ['python3', 'start-training.py', data_dir, '--ensure-gpu']
    training_cmd += remaining_args
    if args.port:
        training_cmd.append('--port=6006')
    training_cmd = ' '.join(training_cmd)

wandb_key = os.environ.get("WANDB_API_KEY")
tmux_cmds = [
    # keep a lot of scrollback history in case it's useful
    "tmux set-option -g history-limit 100000",
    # Activate the virtual environment.
    f"test -d ~/safelife-venv || {src_dir}/gcloud/make-venv.sh",
    "source ~/safelife-venv/bin/activate",
    "python -c 'import torch, sys ; sys.exit(0 if torch.cuda.is_available() else 1)' || exit"
    "echo $PATH",  # for debugging

    # Create a "current_job" directory.
    f"ln -nsf {src_dir} ~/current_job",

    # Install dependencies.
    "sudo apt-get install ffmpeg --yes",
    f"pip install -U wandb",
    f"pip install -r {src_dir}/requirements.txt",

    # Log in to wandb.
    f"wandb login {wandb_key}" if wandb_key else "",

    # Start training!
    f"cd {src_dir}", training_cmd,
]
if args.no_shutdown:
    # Just exit to bash at the end of the run.
    tmux_cmds.append("bash -il")
else:
    # Shut down the machine, but keep ssh available in case we want to
    # quickly re-enter and cancel the shutdown. Also exit to bash.
    tmux_cmds += [
        "sudo shutdown +3",
        "sudo rm -f /run/nologin",
        "bash -il",
    ]

# Run the script remotely, tunneling tensorboard to the specified port.
# Use tmux to prevent it from dying on hangup. Note that if this session is
# already running, tmux should prevent us from running it again.
sp.run(
    [ssh_cmd, args.instance_name] +
    (['-L', args.port + ':localhost:6006'] if args.port else []) +
    [f"tmux new-session -s {args.job_name} \"{'; '.join(tmux_cmds)}\""]
)
