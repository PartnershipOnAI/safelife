#!/usr/bin/env python3

import os
import argparse
import subprocess as sp
import shutil
import tempfile
import textwrap
import uuid


parser = argparse.ArgumentParser(description=textwrap.dedent("""
    Start a new SafeLife training run.

    This performs the following steps:

    1. Clone the repo to a new temporary folder.
    2. Copy all source files over to the remote machine using rsync.
    3. ssh into the remote machine and
        a. create an alias to the soon-to-be-created data folder;
        b. start a tmux session that shares the training job's name;
        c. listens on the appropriate local port for tensorboard updates;
        d. starts training via the `start-training` script.

    Note that the `start-training` script will shut down the remote instance
    with a 10 minute lag before the script exits, whether via error or normal
    completion. The shutdown is there to prevent machine from idling and
    running up large bills, while the lag is designed so that it's possible
    to abort the shutdown by sshing into the remote machine and running
    `sudo shutdown -c`. This comes in handy when the script fails at startup
    due to user error or a bug.
    """), formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument('instance_name', help="name of the gcloud instance")
parser.add_argument('job_name', help="a unique name for this training job")
parser.add_argument('--port', help="local port used to monitor tensorboard")
parser.add_argument('--commit-id',
    help="If supplied, the job is run from the specified commit id (or tag or "
    "branch). Otherwise, changes in the working tree are stashed and the "
    "job is run from the stashed commit. That is, the job is run using the "
    "code as it currently exists, minus any untracked files.")
parser.add_argument('--tag', nargs='?', const="auto",
    help="If set, the commit is given the specified tag. "
    "If auto, the tag is autogenerated with the form 'job-[job_name]'.")
parser.add_argument('--no-shutdown', action="store_true",
    help="By default, the remote machine will shutdown when the job is "
    "complete. This flag prevents that from happening.")
args, remaining_args = parser.parse_known_args()

src_dir = '~/' + args.job_name
data_dir = '~/{job_name}/data/{job_name}/'.format(job_name=args.job_name)

quiet = {'stdout': sp.DEVNULL, 'stderr': sp.DEVNULL}

# There are a couple files which we typically want to keep out of git(hub),
# but we do want to add them to the remote machine if they're available.

if args.commit_id:
    commit_id = args.commit_id
else:
    result = sp.run("git stash create", shell=True, stdout=sp.PIPE, check=True)
    commit_id = result.stdout.decode()
    if not commit_id:
        # means there were no stashed chages. Use HEAD instead.
        result = sp.run("git rev-parse HEAD", shell=True, stdout=sp.PIPE)
        commit_id = result.stdout.decode()

print(f"Setting job to commit id '{commit_id}'...")

if args.tag == 'auto':
    git_tag = f"job-{args.job_name}"
elif args.tag:
    git_tag = args.tag
else:
    # create a temporary tag
    git_tag = f'job-{uuid.uuid4()}'
result = sp.run(f"git tag {git_tag} {commit_id}", shell=True, **quiet)
if result.returncode == 128:
    print(f"Tag '{git_tag}' already exists. Aborting.")
    exit(1)
elif result.returncode != 0:
    print(f"Failed to add tag '{git_tag}'.")
    exit(1)
else:
    print(f"Adding tag '{git_tag}'...")


# Copy over the data
safety_dir = os.path.abspath(os.path.join(__file__, '../../'))
ssh_cmd = os.path.abspath(os.path.join(__file__, '../ssh'))

print("Cloning temporary repo...")

try:
    tmp_repo = tempfile.TemporaryDirectory(prefix="safelife-job-")
    result = sp.run(
        f"git clone --no-local --single-branch --branch {git_tag} . {tmp_repo.name}",
        shell=True, **quiet)
    if result.returncode != 0:
        print("Error cloning the repo.")
        exit(1)

# Also copy over some files that are not (necessarily) in source control.
    extra_files = [
        'run-info.md',
        'run-notes.txt',
        'wandb/settings',
    ]
    for extra_file in extra_files:
        src_file = os.path.join(safety_dir, extra_file)
        dst_file = os.path.join(tmp_repo.name, extra_file)
        if os.path.exists(src_file):
            os.makedirs(os.path.dirname(dst_file), exist_ok=True)
            shutil.copyfile(src_file, dst_file)

    print("Syncing repo to the cloud...")
    result = sp.run([
        'rsync', '--rsh', ssh_cmd, '-ra',
        tmp_repo.name + '/', args.instance_name + ':' + src_dir])
    if result.returncode != 0:
        exit(1)
finally:
    if not args.tag:
        print("Removing temporary tag...")
        sp.run(f"git tag -d {git_tag}", shell=True, **quiet)
    print("Cleaning temporary repo...")
    tmp_repo.cleanup()

print("Starting job...")
training_cmd = ['./start-training', data_dir, '--ensure-gpu']
training_cmd += remaining_args
if not args.no_shutdown:
    training_cmd.append('--shutdown')
tmux_cmds = [
    # Load all of the environment variables as if we were running from a
    # login shell. Importantly, this uses the Anaconda versions of the
    # python environment.
    "source /etc/profile",
    "source ~/.bashrc",

    # Create a "current_job" directory. This is handy for re-entering the job.
    f"ln -nsf {src_dir} ~/current_job",

    # Install dependencies.
    "sudo apt-get install ffmpeg --yes",
    f"pip install --user -r {src_dir}/requirements.txt",

    # Run the training script!
    f"cd {src_dir}", ' '.join(training_cmd),

    # Finally, exit to bash. This is very useful for debugging.
    "bash -il",
]

# Run the script remotely, tunneling tensorboard to the specified port.
# Use tmux to prevent it from dying on hangup. Note that if this session is
# already running, tmux should prevent us from running it again.
sp.run(
    [ssh_cmd, args.instance_name] +
    (['-L', args.port + ':localhost:6006'] if args.port else []) +
    [f"tmux new-session -s {args.job_name} \"{'; '.join(tmux_cmds)}\""]
)
