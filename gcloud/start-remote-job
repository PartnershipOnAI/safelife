#!/usr/bin/env python3

import os
import argparse
import subprocess
import textwrap
import glob


parser = argparse.ArgumentParser(description=textwrap.dedent("""
    Start a new SafeLife training run.

    This performs the following steps:

    1. Copy all source files over to the remote machine using rsync.
    2. ssh into the remote machine and
        a. create an alias to the soon-to-be-created data folder;
        b. start a tmux session that shares the training job's name;
        c. listens on the appropriate local port for tensorboard updates;
        d. starts training via the `start-training` script.

    Note that the `start-training` script will shut down the remote instance with a
    10 minute lag before the script exits, whether via error or normal
    completion. The shutdown is there to prevent machine from idling and
    running up large bills, while the lag is designed so that it's possible
    to abort the shutdown by sshing into the remote machine and running
    `sudo shutdown -c`. This comes in handy when the script fails at startup
    due to user error or a bug.
    """), formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument('instance_name', help="name of the gcloud instance")
parser.add_argument('job_name', help="a unique name for this training job")
parser.add_argument('--port', default='6006',
    help="local port used to monitor tensorboard")
args, remaining_args = parser.parse_known_args()

src_dir = '~/' + args.job_name
data_dir = '~/{job_name}/data/{job_name}/'.format(job_name=args.job_name)

# Copy over the data
safety_dir = os.path.abspath(os.path.join(__file__, '../../'))
ssh_cmd = os.path.abspath(os.path.join(__file__, '../ssh'))

with open('tmp_files_list', 'w') as f:
    f.write(
        # Copy over all txt and md files.
        # The only one that's really needed is requirements.txt,
        # but it can be handy to copy over notes in a text file as well.
        '\n'.join(glob.glob('*.txt')) + '\n' +
        '\n'.join(glob.glob('*.md')) + '\n' +
        '\n'.join(glob.glob('*.py')) + '\n'
        'setup.cfg\n'
        'MANIFEST.in\n'
        'start-training\n'
        'training/\n'
        'safelife/\n'
        'wandb/settings\n'
    )
with open('tmp_exclude_list', 'w') as f:
    f.write(
        '*.so\n'
        '__pycache__\n'
    )
result = subprocess.run([
    'rsync', '--rsh', ssh_cmd, '-ra', '--files-from=tmp_files_list',
    '--exclude-from=tmp_exclude_list',
    '.', args.instance_name + ':' + src_dir])
os.remove('tmp_files_list')
os.remove('tmp_exclude_list')
if result.returncode != 0:
    exit(result.returncode)

# Run the script, tunneling tensorboard to the specified port.
result = subprocess.run([
    ssh_cmd, args.instance_name, '-L', args.port + ':localhost:6006',

    # Set the path variable to include conda, which contains the latest
    # versions of python, torch, etc.
    # This is very specific to our particular gcloud setup.
    "PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:$PATH; "

    # Create a "current_job" directory. This is handy for re-entering the job.
    f"ln -nsf {src_dir} ~/current_job; "

    # Install dependencies.
    "sudo apt-get install ffmpeg --yes; "
    f"pip install -r {src_dir}/requirements.txt; "

    # And run the training script!
    # This uses tmux to prevent it from dying on hangup.
    # Note that if this session is already running, tmux should prevent us
    # from running it again.
    f"tmux new-session -s {args.job_name} "
    f"{src_dir}/start-training {data_dir} --shutdown " + ' '.join(remaining_args)
])
